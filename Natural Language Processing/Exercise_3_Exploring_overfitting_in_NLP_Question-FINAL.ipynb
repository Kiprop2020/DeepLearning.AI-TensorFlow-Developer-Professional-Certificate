{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hmA6EzkQJ5jt"},"outputs":[],"source":["### ATTENTION: Please do not alter any of the provided code in the exercise. Only add your own code where indicated\n","# ATTENTION: Please do not add or remove any cells in the exercise. The grader will check specific cells based on the cell position.\n","# ATTENTION: Please use the provided epoch values when training.\n","\n","import json\n","import tensorflow as tf\n","import csv\n","import random\n","import numpy as np\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import regularizers\n","from os import getcwd\n","\n","embedding_dim = 100\n","max_length = 16\n","trunc_type='post'\n","padding_type='post'\n","oov_tok = \"<OOV>\"\n","training_size= 160000 #Your dataset size here. Experiment using smaller values (i.e. 16000), but don't forget to train on at least 160000 to see the best effects\n","test_portion=.1\n","\n","corpus = []\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bM0l_dORKqE0"},"outputs":[],"source":["\n","# Note that I cleaned the Stanford dataset to remove LATIN1 encoding to make it easier for Python CSV reader\n","# You can do that yourself with:\n","# iconv -f LATIN1 -t UTF8 training.1600000.processed.noemoticon.csv -o training_cleaned.csv\n","\n","path_training_cleaned = f\"{getcwd()}/../tmp2/training_cleaned.csv\"\n","\n","num_sentences = 0\n","\n","with open(path_training_cleaned) as csvfile:\n","    reader = csv.reader(csvfile, delimiter=',')\n","    for row in reader:\n","        list_item=[]\n","        # YOUR CODE HERE\n","        list_item.append(row[5])\n","        label = row[0]\n","        if label == '0':\n","            list_item.append(0)\n","        else: \n","            list_item.append(1)\n","        num_sentences += 1\n","        corpus.append(list_item)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3kxblBUjEUX-","outputId":"8967ca9e-1d29-4e20-9d6e-8a21eaa5cec6"},"outputs":[{"name":"stdout","output_type":"stream","text":["1600000\n","1600000\n","[\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]\n"]}],"source":["print(num_sentences)\n","print(len(corpus))\n","print(corpus[1])\n","\n","# Expected Output:\n","# 1600000\n","# 1600000\n","# [\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohOGz24lsNAD"},"outputs":[],"source":["sentences=[]\n","labels=[]\n","random.shuffle(corpus)\n","for x in range(training_size):\n","    sentences.append(corpus[x][0])\n","    labels.append(corpus[x][1])\n","\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","\n","word_index = tokenizer.word_index\n","vocab_size=len(word_index)\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","split = int(test_portion * training_size)\n","\n","test_sequences = padded[:split]\n","training_sequences = padded[split:]\n","test_labels = labels[:split]\n","training_labels = labels[split:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIrtRem1En3N","outputId":"4ee2e08d-34ab-41bc-9b6c-4cfec9fbb692"},"outputs":[{"name":"stdout","output_type":"stream","text":["138388\n","1\n"]}],"source":["print(vocab_size)\n","print(word_index['i'])\n","# Expected Output\n","# 138858\n","# 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1zdgJkusRh0"},"outputs":[],"source":["# Note this is the 100 dimension version of GloVe from Stanford\n","path_glove = f\"{getcwd()}/../tmp2/glove.6B.100d.txt\"\n","embeddings_index = {};\n","with open(path_glove) as f:\n","    for line in f:\n","        values = line.split();\n","        word = values[0];\n","        coefs = np.asarray(values[1:], dtype='float32');\n","        embeddings_index[word] = coefs;\n","\n","embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word);\n","    if embedding_vector is not None:\n","        embeddings_matrix[i] = embedding_vector;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71NLk_lpFLNt","outputId":"1e622f26-0046-47b3-b555-95b8b7c09130"},"outputs":[{"name":"stdout","output_type":"stream","text":["138389\n"]}],"source":["print(len(embeddings_matrix))\n","# Expected Output\n","# 138859"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKKvbuEBOGFz","outputId":"8253ed10-21a0-489c-8beb-31891230b544"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 16, 100)           13838900  \n","_________________________________________________________________\n","conv1d_2 (Conv1D)            (None, 12, 64)            32064     \n","_________________________________________________________________\n","global_max_pooling1d_1 (Glob (None, 64)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                4160      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 13,875,189\n","Trainable params: 36,289\n","Non-trainable params: 13,838,900\n","_________________________________________________________________\n","Train on 144000 samples, validate on 16000 samples\n","Epoch 1/15\n","144000/144000 - 128s - loss: 0.5575 - accuracy: 0.7077 - val_loss: 0.5374 - val_accuracy: 0.7238\n","Epoch 2/15\n","144000/144000 - 122s - loss: 0.5094 - accuracy: 0.7446 - val_loss: 0.5213 - val_accuracy: 0.7374\n","Epoch 3/15\n","144000/144000 - 123s - loss: 0.4867 - accuracy: 0.7608 - val_loss: 0.5160 - val_accuracy: 0.7414\n","Epoch 4/15\n","144000/144000 - 124s - loss: 0.4685 - accuracy: 0.7730 - val_loss: 0.5170 - val_accuracy: 0.7427\n","Epoch 5/15\n","144000/144000 - 125s - loss: 0.4528 - accuracy: 0.7834 - val_loss: 0.5194 - val_accuracy: 0.7434\n","Epoch 6/15\n","144000/144000 - 125s - loss: 0.4409 - accuracy: 0.7896 - val_loss: 0.5226 - val_accuracy: 0.7406\n","Epoch 7/15\n","144000/144000 - 126s - loss: 0.4292 - accuracy: 0.7985 - val_loss: 0.5366 - val_accuracy: 0.7406\n","Epoch 8/15\n","144000/144000 - 126s - loss: 0.4175 - accuracy: 0.8041 - val_loss: 0.5427 - val_accuracy: 0.7378\n","Epoch 9/15\n","144000/144000 - 126s - loss: 0.4090 - accuracy: 0.8089 - val_loss: 0.5830 - val_accuracy: 0.7179\n","Epoch 10/15\n","144000/144000 - 124s - loss: 0.3999 - accuracy: 0.8139 - val_loss: 0.5554 - val_accuracy: 0.7353\n","Epoch 11/15\n","144000/144000 - 122s - loss: 0.3915 - accuracy: 0.8189 - val_loss: 0.5617 - val_accuracy: 0.7353\n","Epoch 12/15\n","144000/144000 - 126s - loss: 0.3846 - accuracy: 0.8221 - val_loss: 0.5821 - val_accuracy: 0.7314\n","Epoch 13/15\n","144000/144000 - 124s - loss: 0.3781 - accuracy: 0.8259 - val_loss: 0.5793 - val_accuracy: 0.7339\n","Epoch 14/15\n","144000/144000 - 122s - loss: 0.3714 - accuracy: 0.8299 - val_loss: 0.5916 - val_accuracy: 0.7296\n","Epoch 15/15\n","144000/144000 - 124s - loss: 0.3665 - accuracy: 0.8311 - val_loss: 0.5912 - val_accuracy: 0.7305\n","Training Complete\n"]}],"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n","    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n","    tf.keras.layers.GlobalMaxPooling1D(),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","             metrics=['accuracy'])\n","model.summary()\n","\n","history = model.fit(training_sequences, np.array(training_labels), epochs=15, batch_size=128, validation_data=(test_sequences, np.array(test_labels)), verbose=2)\n","\n","print(\"Training Complete\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qxju4ItJKO8F"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['accuracy']\n","val_acc=history.history['val_accuracy']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r')\n","plt.plot(epochs, val_acc, 'b')\n","plt.title('Training and validation accuracy')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n","\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot(epochs, loss, 'r')\n","plt.plot(epochs, val_loss, 'b')\n","plt.title('Training and validation loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend([\"Loss\", \"Validation Loss\"])\n","\n","plt.figure()"]}],"metadata":{"accelerator":"GPU","colab":{"name":"Exercise_3_Exploring_overfitting_in_NLP_Question-FINAL.ipynb","provenance":[]},"coursera":{"course_slug":"natural-language-processing-tensorflow","graded_item_id":"dtMI9","launcher_item_id":"WQgnj"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":0}